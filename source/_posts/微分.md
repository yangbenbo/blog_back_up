---
title: 微分
date: 2024-05-03 10:21:14
categories:
- 数学
tags:
- 微积分
mathjax: true
---

# 微分
微积分(Calculus[^1])包括微分(Differential calculus[^2], 研究变化率)和积分(Integral calculus, 研究曲线下的面积),这里主要总结微分的基本性质(Differentiation rules[^3]).简单使用可以用[在线网站](https://www.matrixcalculus.org/)计算常用导数

微分通常用于寻找极值,微分方程是用于描述自然现象的基础.
导数对应函数因变量的变化率,即函数在对应点的斜率,即两个变化率相除进行逼近,变化无穷小则得到对应点的导数,基本函数都可以通过这种方式进行求导
$$
\frac{dy}{dx}=f'(x)=\lim_{\Delta x\to0}\frac{f(x+\Delta x)-f(x)}{\Delta x}
$$

## 微分规则
1. 常数的导数为0(斜率为0)
$f(x)=c,c\in \mathbb{R}$, 则有$\frac{df}{dx}=0$
2. 微分计算是线性的
简单推导如下
$$
\begin{align}
\frac{d(af+bg)}{dx} & = \frac{af(x+\Delta x)+bg(x+\Delta x)-(af(x)+bg(x))}{\Delta x}\nonumber\\
&=\frac{a(f(x+\Delta x)-f(x))}{\Delta x}+\frac{b(g(x+\Delta x)-g(x))}{\Delta x} \nonumber\\
&= a\frac{df}{dx}+b\frac{dg}{dx} \nonumber
\end{align}
$$
3. product rule
可以类似上面基本定义进行推导
$$\frac{d(fg)}{dx}=g\frac{df}{dx}+f\frac{dg}{dx}$$
4. chain rule
$$\frac{df(g(x))}{dx}=\lim_{x\to a}\frac{f(g(x))-f(g(a))}{x-a}=\lim_{x\to a}\frac{f(g(x))-f(g(a))}{g(x)-g(a)}\cdot\frac{g(x)-g(a)}{x-a}=\frac{df}{dg}\frac{dg}{dx}$$
5. inverse function rule
$f(x)=y$,逆函数$f^{-1}(y)=x$, 对应导数就是斜率,只是分母不同,可以直接看成两个无穷小相除的导数
$$
\frac{dx}{dy}=\frac1{\frac{dy}{dx}}
$$

## 矩阵微分
矩阵微分只是多变量微分的一种紧凑的表示方式,极大简化了寻找多变量极值,求解微分方程的难度



### 爱因斯坦求和约定
#### 定义
https://en.wikipedia.org/wiki/Einstein_notation


单独项中有个索引重复2次,意味着对应项为所有这个索引值对应项的求和,
- 内积
$$\mathbf{u}\cdot\mathbf{v}=u_{j}v^{j}$$
- 矩阵-向量乘积
$$\mathbf{u}_i=(\mathbf{A}\mathbf{v})_i=\sum_{j=1}^NA_{ij}v_j$$
对应求和约定为
$$u^i=A^i{}_jv^j$$
- 矩阵乘法
$$\mathbf{C}_{ik}=(\mathbf{AB})_{ik}=\sum_{j=1}^NA_{ij}B_{jk}$$
对应
$$C^i{}_k=A^i{}_jB^j{}_k$$


#### numpy `einsum`使用

对应numpy对应函数[`einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html), 针对`ij,jk->ik`, `->`左边为输入,右边为输出,参考 https://ajcr.net/Basic-guide-to-einsum/

- Repeating letters between input arrays means that values along those axes will be multiplied together. The products make up the values for the output array.
- Omitting a letter from the output means that values along that axis will be summed.

一维向量
| **Call signature**  | **NumPy equivalent** |            **Description**             |
| :-----------------: | :------------------: | :------------------------------------: |
|     `('i', A)`,`('i->i', A)`       |         `A`          |          returns a view of A           |
|    `('i->', A)`     |       `sum(A)`       |          sums the values of A          |
| `('i,i->i', A, B)`  |       `A * B`        | element-wise multiplication of A and B |
|   `('i,i', A, B)`   |    `inner(A, B)`     |        inner product of A and B        |
| `('i,j->ij', A, B)` |    `outer(A, B)`     |        outer product of A and B        |


二维矩阵
| **Call signature**      | **NumPy equivalent**      | **Description**                          |
| ----------------------- | ------------------------- | ---------------------------------------- |
| `('ij', A)`,`('ij->ij', A)`             | `A`                       | returns a view of A                      |
| `('ji', A)`             | `A.T`                     | view transpose of A                      |
| `('ii->i', A)`          | `diag(A)`                 | view main diagonal of A                  |
| `('ii', A)`             | `trace(A)`                | sums main diagonal of A                  |
| `('ij->', A)`           | `sum(A)`                  | sums the values of A                     |
| `('ij->j', A)`          | `sum(A, axis=0)`          | sum down the columns of A (across rows)  |
| `('ij->i', A)`          | `sum(A, axis=1)`          | sum horizontally along the rows of A     |
| `('ij,ij->ij', A, B)`   | `A * B`                   | element-wise multiplication of A and B   |
| `('ij,ji->ij', A, B)`   | `A * B.T`                 | element-wise multiplication of A and B.T |
| `('ij,jk', A, B)`       | `dot(A, B)`               | matrix multiplication of A and B         |
| `('ij,kj->ik', A, B)`   | `inner(A, B)`             | inner product of A and B                 |

实际使用中可以通过基本求和方式进行推导,最后化简为爱因斯坦求和约定方式再实现,比如
```
import numpy as np
A = np.random.random((3, 3))
B = np.random.random((3, 3))

print(A.T @ B - np.einsum('ji,jk->ik', A,B))
print(A @ B.T - np.einsum('ij,kj->ik', A,B))
```
### 标量对矩阵求导[^4]
标量$f$对矩阵$X$求导定义为逐个元素求导并排列成矩阵, ${\frac{\partial f}{\partial X}}=\left[{\frac{\partial f}{\partial X_{ij}}}\right]$

1元函数微分 $df=f'(x)dx$, 多元函数$df=\sum_{i=1}^n\frac{\partial f}{\partial x_i}dx_i=\frac{\partial f}{\partial x}^Td\boldsymbol{x}=(\nabla_x f(x))^Td\boldsymbol{x}$, 全微分是梯度[^5]向量(梯度和全微分是相互转置的关系)与微分向量的内积.受此启发, 针对矩阵微分可以是
$$
df=\sum_{i=1}^m\sum_{j=1}^n\frac{\partial f}{\partial X_{ij}}dX_{ij}=\mathrm{tr}\left(\frac{\partial f}{\partial X}^TdX\right)
$$
$\text{tr}(A^TB)$是矩阵$A,B$的内积,这里表示全微分$df$是导数$\frac{\partial f}{\partial X}(m \times n)$与微分矩阵$dX(m \times n)$的内积
**标量对矩阵求导方法:**
- 若标量函数$f$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$f$求微分，再使用迹技巧给$df$套上迹并将其它项交换至$dX$左侧，对照导数与微分的联系$df=\mathrm{tr}\left(\frac{\partial f}{\partial X}^TdX\right)$, 即能得到导数
- 若矩阵退化成向量, 则可以直接使用导数与微分的联系$df=\frac{\partial f}{\partial x}^Td\boldsymbol{x}$对比得到

#### 矩阵微分常用法则
1. 加减法$d(X\pm Y)=dX \pm dY $, $d(XY)=(dX)Y+X(dY)$, $d(X^T)=(dX)^T$, $d\text {tr} X=\text{tr}dX$
可以使用爱因斯坦求和约定进行推导更方便, 比如矩阵乘法微分:
矩阵乘法表示$Z_{ik}=X_{ij}Y_{jk}$, 对应$dZ_{ik}=d(X_{ij}Y_{jk})=dX_{ij}Y_{jk}+X_{ij}dY_{jk}$, 其中第2个等号求和的微分为每项微分的和,即加减法对应的法则, 最后还原成矩阵表示就是$d(XY)=(dX)Y+X(dY)$
2. 逆, $dX^{-1}=-X^{-1}dXX^{-1}$, 可以通过$d(XX^{-1})=0$展开证明
3. 行列式, $d|X|=\mathrm{tr}(X^adX)$, 其中$X^{a}$为$X$伴随矩阵,可逆时为$d|X|=|X|\mathrm{tr}(X^{-1}dX)$, **待详细证明**
4. 逐元素乘, $d(X\odot Y)=dX\odot Y+X\odot dY$

迹基本性质
1. 标量, $a=\text{tr}(a)$
2. 转置, $\text{tr}(A^T)=\text{tr}(A)$
3. 线性, $\text{tr}(A\pm B)=\text{tr}(A) \pm \text{tr}(B)$
4. $\text{tr}(AB)=\text{tr}(BA)$, 证明如下, $\text{tr}(AB)=A_{ij}B_{ji}$, $\text{tr}(BA)=B_{ij}A_{ji}=A_{ij}B_{ji}$, 交换累加求和顺序可以证明两个等价的
5. 矩阵乘法/逐元素乘法交换, $\mathrm{tr}(A^T(B\odot C))=\mathrm{tr}((A\odot B)^TC)$, 可以使用爱因斯坦求和证明结果都是$\sum_{i,j}A_{ij}B_{ij}{C_{ij}}$

#### 线性回归微分示例
线性方程组$Ax=b$, 其中$A\in \mathbb{R^{m \times n}}, b\in \mathbb{R^m}, m > n$, 约束比变量多通常无解, 但是可以找一个最小二乘解,即
$$
\min_{x\in\mathbb{R}^n}f(x)=\|Ax-b\|^2=(Ax-b)^T(Ax-b)
$$
则有
$$
df=(Adx)^T(Ax-b)+(Ax-b)^TAdx=2(Ax-b)^TAdx
$$
得到$\frac{\partial f}{\partial x}=(2(Ax-b)^TA)^T$, 对照$df=\frac{\partial f}{\partial x}^Td\boldsymbol{x}$, 导数为0,即
$A^TAx=A^Tb$, 对应如果$\text{rank} (A)=n$, 则$A^TA$可逆,对应$x=(A^TA)^{-1}A^Tb$为最小二乘解



[^1]: https://en.wikipedia.org/wiki/Calculus
[^2]: https://en.wikipedia.org/wiki/Differential_calculus
[^3]: https://en.wikipedia.org/wiki/Differentiation_rules
[^4]: https://zhuanlan.zhihu.com/p/24709748
[^5]: https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6
